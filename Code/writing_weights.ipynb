{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview\n",
    "\n",
    "**The purpose of this file is to compile all of the different weight files that I wanted to test for this manuscript. The file covers a few different weighting schemes that were developed and ran in succession as the project evolved but it should be clear which cells perform what methods.**\n",
    "\n",
    "**The only thing to note is that not all weighting methods / perturbations appear in the manuscript. In particular, for each weighting method I adapted uniform weight controls as a pair such that each uneven weighting scheme (with a different eff no of sequences) had a paired uniform weight file that added up to the same eff no of sequences. At least for the moment, all of this code remains but lead nowhere**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import glob\n",
    "from matplotlib import pyplot as plt\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###Including my custom file\n",
    "import supporting_functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###And what may be a non-standard libraries that you'll have to install\n",
    "from Bio import SeqIO, Phylo\n",
    "import Levenshtein "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Writing all weights\n",
    "\n",
    "This first section is going to write a LOT of weight files!\n",
    "\n",
    "You should have the below directory structure set up and ready to go.\n",
    "\n",
    "The next section will specifically write GSC, ACL, and HH weights. For each there will be a mean- and max-scaled file as well as a set of uniform weights corresponding to the max-file. Additionally I'm writing a plain old uniform file to start. \n",
    "\n",
    "So for each protein in the fasta_directory these cells directly below will write 10 separate weight files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fasta_directory = '../Data/psicov150_aln_pdb/aln_fasta_max1k/'\n",
    "tree_directory = '../Data/psicov150_aln_pdb/aln_fasta_max1k_iqtree/'\n",
    "\n",
    "weight_output_dir = '../Data/weights/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for fasta_file in sorted(glob.glob(fasta_directory+'*.fasta'))[:]:\n",
    "    prot_name = fasta_file.split('/')[-1].split('.')[0]\n",
    "    #First read in sequence data\n",
    "    records = list(SeqIO.parse(fasta_file, 'fasta'))\n",
    "    #And the tree\n",
    "    tree_loc = tree_directory+'{}.fasta.treefile'.format(prot_name)\n",
    "    try:\n",
    "        tree = Phylo.read(tree_loc, rooted=False, format='newick')\n",
    "    except FileNotFoundError:\n",
    "        continue\n",
    "    #Root the tree with my mid-point algorithm\n",
    "    tree = supporting_functions.MP_root(tree)\n",
    "    \n",
    "    #First write basic uniform weights    \n",
    "    weight_val = 1.0\n",
    "    with open(weight_output_dir+'{}_uniform.weights'.format(prot_name),'w') as outfile:\n",
    "        for record in records:\n",
    "            outfile.write('{}\\n'.format(weight_val))\n",
    "    ##############################################################################\n",
    "    #Move on to GSC weights\n",
    "    ##############################################################################\n",
    "    print('####################GSC', prot_name, len(records), len(tree.get_terminals()))    \n",
    "    weights_dict =  weighting_methods.calc_GSC_weights(tree)\n",
    "    weights_dict_id = {}\n",
    "    for i,j in weights_dict.items():\n",
    "        weights_dict_id[i.name] = j\n",
    "    \n",
    "    #Mean scale these\n",
    "    mean_GSC = np.mean(list(weights_dict_id.values()))\n",
    "    keys, vals = zip(*list(weights_dict_id.items()))\n",
    "    vals = np.array(vals)/mean_GSC\n",
    "    new_dict = dict(zip(keys, vals))\n",
    "    \n",
    "    #And write to file\n",
    "    with open(weight_output_dir+'{}_GSC_{}.weights'.format(prot_name, 'meanScale'),'w') as outfile:\n",
    "        for record in records:\n",
    "            outfile.write('{}\\n'.format(new_dict[record.id]))\n",
    "    print(np.sum(list(new_dict.values())))   \n",
    "    \n",
    "    #Max scale them\n",
    "    max_GSC = np.max(list(weights_dict_id.values()))\n",
    "    keys, vals = zip(*list(weights_dict_id.items()))\n",
    "    vals = np.array(vals)/max_GSC\n",
    "    new_dict = dict(zip(keys, vals))\n",
    "    \n",
    "    #And write to file\n",
    "    with open(weight_output_dir+'{}_GSC_{}.weights'.format(prot_name, 'maxScale'),'w') as outfile:\n",
    "        for record in records:\n",
    "            outfile.write('{}\\n'.format(new_dict[record.id]))\n",
    "    print(np.sum(list(new_dict.values())))   \n",
    "    \n",
    "    ### Write corresponding uniform for max scale\n",
    "    weight_val = np.sum(list(new_dict.values()))/len(records)\n",
    "    with open(weight_output_dir+'{}_GSC_maxScale_uniform.weights'.format(prot_name),'w') as outfile:\n",
    "        for record in records:\n",
    "            outfile.write('{}\\n'.format(weight_val))\n",
    "    \n",
    "    ##############################################################################\n",
    "    #ACL weights (reload and re-root the tree)\n",
    "    ##############################################################################\n",
    "    try:\n",
    "        tree = Phylo.read(tree_loc, rooted=False, format='newick')\n",
    "    except FileNotFoundError:\n",
    "        continue\n",
    "    tree = supporting_functions.MP_root(tree)\n",
    "    print('####################ACL', prot_name, len(records), len(tree.get_terminals()))\n",
    "\n",
    "    #Get ACL weights\n",
    "    weights_dict, new_tree =  weighting_methods.calc_ACL_weights(tree)\n",
    "    weights_dict_id = {}\n",
    "    for i,j in weights_dict.items():\n",
    "        weights_dict_id[i.name] = j\n",
    "        \n",
    "    #Mean scale\n",
    "    mean_ACL = np.mean(list(weights_dict_id.values()))\n",
    "    keys, vals = zip(*list(weights_dict_id.items()))\n",
    "    vals = np.array(vals)/mean_ACL\n",
    "    new_dict = dict(zip(keys, vals))\n",
    "    \n",
    "    #And write\n",
    "    with open(weight_output_dir+'{}_ACL_{}.weights'.format(prot_name, 'meanScale'),'w') as outfile:\n",
    "        for record in records:\n",
    "            outfile.write('{}\\n'.format(new_dict[record.id]))\n",
    "    print(np.sum(list(new_dict.values())))\n",
    "    \n",
    "    #Max scale\n",
    "    max_ACL = np.max(list(weights_dict_id.values()))\n",
    "    keys, vals = zip(*list(weights_dict_id.items()))\n",
    "    vals = np.array(vals)/max_ACL\n",
    "    new_dict = dict(zip(keys, vals))\n",
    "    \n",
    "    #And write\n",
    "    with open(weight_output_dir+'{}_ACL_{}.weights'.format(prot_name, 'maxScale'),'w') as outfile:\n",
    "        for record in records:\n",
    "            outfile.write('{}\\n'.format(new_dict[record.id]))\n",
    "    print(np.sum(list(new_dict.values()))) \n",
    "    \n",
    "    ### Write corresponding uniform for max scale\n",
    "    weight_val = np.sum(list(new_dict.values()))/len(records)\n",
    "    with open(weight_output_dir+'{}_ACL_maxScale_uniform.weights'.format(prot_name),'w') as outfile:\n",
    "        for record in records:\n",
    "            outfile.write('{}\\n'.format(weight_val))\n",
    "            \n",
    "            \n",
    "    ##############################################################################\n",
    "    #HH weights\n",
    "    ##############################################################################\n",
    "    print('####################HH', prot_name, len(records), len(tree.get_terminals()))\n",
    "    weights_dict = weighting_methods.calc_HH_weights(records)\n",
    "    #Mean scale\n",
    "    weights_array = np.array(list(weights_dict.values()))    \n",
    "    mean_HH = np.mean(list(weights_dict.values()))\n",
    "    keys, vals = zip(*list(weights_dict.items()))\n",
    "    vals = np.array(vals)/mean_HH\n",
    "    new_dict = dict(zip(keys, vals))\n",
    "    #Write \n",
    "    with open(weight_output_dir+'{}_HH_{}.weights'.format(prot_name, 'meanScale'),'w') as outfile:\n",
    "        for record in records:\n",
    "            outfile.write('{}\\n'.format(new_dict[record.id]))\n",
    "    print(np.sum(list(new_dict.values())))\n",
    "    #Max scale\n",
    "    max_HH = np.max(list(weights_dict.values()))\n",
    "    keys, vals = zip(*list(weights_dict.items()))\n",
    "    vals = np.array(vals)/max_HH\n",
    "    new_dict = dict(zip(keys, vals))\n",
    "    #Write\n",
    "    with open(weight_output_dir+'{}_HH_{}.weights'.format(prot_name, 'maxScale'),'w') as outfile:\n",
    "        for record in records:\n",
    "            outfile.write('{}\\n'.format(new_dict[record.id]))\n",
    "    print(np.sum(list(new_dict.values())))\n",
    "    \n",
    "    #And finally write corresponding uniform for max scale\n",
    "    weight_val = np.sum(list(new_dict.values()))/len(records)\n",
    "    with open(weight_output_dir+'{}_HH_maxScale_uniform.weights'.format(prot_name),'w') as outfile:\n",
    "        for record in records:\n",
    "            outfile.write('{}\\n'.format(weight_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Print prot names to make a bash array**\n",
    "\n",
    "(just a little something I used in my scripting, not necessary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "prot_names = []\n",
    "for fasta_file in sorted(glob.glob('../../Phylogenetic_couplings/Data/'\n",
    "                                   'psicov150_aln_pdb/aln_fasta_max1k/*.fasta'))[:]:\n",
    "    #Read in sequence data and tree\n",
    "    records = list(SeqIO.parse(fasta_file, 'fasta'))\n",
    "    prot_name = fasta_file.split('/')[-1].split('.')[0]\n",
    "    prot_names.append(prot_name)\n",
    "for i in prot_names:\n",
    "    print('\"{}\"'.format(i), end=\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This section uses the RelTime algorithm\n",
    "\n",
    "Specifically to scale the trees and re-do calculations for GSC and ACL weights.\n",
    "\n",
    "Code functions basically the same as above, and writes 6 files for each protein: mean and max scaled GSC and ACL weights, and corresponding uniform weight controls for the two max files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fasta_directory = '../Data/psicov150_aln_pdb/aln_fasta_max1k/'\n",
    "tree_directory = '../Data/psicov150_aln_pdb/aln_fasta_max1k_iqtree/'\n",
    "\n",
    "weight_output_dir = '../Data/weights/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for fasta_file in sorted(glob.glob(fasta_directory+'*.fasta'))[:]:\n",
    "    #Read in sequence data and tree\n",
    "    records = list(SeqIO.parse(fasta_file, 'fasta'))\n",
    "    prot_name = fasta_file.split('/')[-1].split('.')[0]\n",
    "    tree_loc = tree_directory+'{}.fasta.treefile'.format(prot_name)\n",
    "    try:\n",
    "        tree = Phylo.read(tree_loc, rooted=False, format='newick')\n",
    "    except FileNotFoundError:\n",
    "        continue  \n",
    "    tree = supporting_functions.MP_root(tree)\n",
    "    ##############################################\n",
    "    ###RelTime transformation\n",
    "    ##############################################\n",
    "    tree.root.branch_length = 0.0\n",
    "    tree = supporting_functions.rel_time_AJH(tree)\n",
    "    for node in tree.get_terminals() + tree.get_nonterminals():\n",
    "        if node == tree.root:\n",
    "            continue\n",
    "        node.branch_length = node.branch_length/node.rate\n",
    "    tree.root.branch_length = None\n",
    "\n",
    "\n",
    "    ##############################################################################\n",
    "    #GSC weights as above\n",
    "    ##############################################################################\n",
    "    print('####################GSC', prot_name, len(records), len(tree.get_terminals()))\n",
    "    weights_dict =  weighting_methods.calc_GSC_weights(tree)\n",
    "    weights_dict_id = {}\n",
    "    for i,j in weights_dict.items():\n",
    "        weights_dict_id[i.name] = j\n",
    "    \n",
    "    #Mean scale them\n",
    "    mean_GSC = np.mean(list(weights_dict_id.values()))\n",
    "    keys, vals = zip(*list(weights_dict_id.items()))\n",
    "    vals = np.array(vals)/mean_GSC\n",
    "    new_dict = dict(zip(keys, vals))\n",
    "    with open(weight_output_dir+'{}_GSC_{}.RelTime.weights'.format(prot_name, 'meanScale'),'w') as outfile:\n",
    "        for record in records:\n",
    "            outfile.write('{}\\n'.format(new_dict[record.id]))\n",
    "    print(np.sum(list(new_dict.values())))  \n",
    "    \n",
    "    #Max scale them\n",
    "    max_GSC = np.max(list(weights_dict_id.values()))\n",
    "    keys, vals = zip(*list(weights_dict_id.items()))\n",
    "    vals = np.array(vals)/max_GSC\n",
    "    new_dict = dict(zip(keys, vals))\n",
    "    with open(weight_output_dir+'{}_GSC_{}.RelTime.weights'.format(prot_name, 'maxScale'),'w') as outfile:\n",
    "        for record in records:\n",
    "            outfile.write('{}\\n'.format(new_dict[record.id]))\n",
    "    print(np.sum(list(new_dict.values())))   \n",
    "    \n",
    "    ### Write corresponding uniform for max scale\n",
    "    weight_val = np.sum(list(new_dict.values()))/len(records)\n",
    "    with open(weight_output_dir+'{}_GSC_maxScale_uniform.RelTime.weights'.format(prot_name),'w') as outfile:\n",
    "        for record in records:\n",
    "            outfile.write('{}\\n'.format(weight_val))\n",
    "    \n",
    "    ##############################################################################\n",
    "    #ACL weights (reload and re-root the tree first)\n",
    "    ##############################################################################\n",
    "    try:\n",
    "        tree = Phylo.read(tree_loc, rooted=False, format='newick')\n",
    "    except FileNotFoundError:\n",
    "        continue  \n",
    "    tree = supporting_functions.MP_root(tree)\n",
    "    ##############################################\n",
    "    ###RelTime transformation\n",
    "    ##############################################\n",
    "    tree.root.branch_length = 0.0\n",
    "    tree = supporting_functions.rel_time_AJH(tree)\n",
    "    for node in tree.get_terminals() + tree.get_nonterminals():\n",
    "        if node == tree.root:\n",
    "            continue\n",
    "        node.branch_length = node.branch_length/node.rate\n",
    "    tree.root.branch_length = None\n",
    "\n",
    "    print('####################ACL', prot_name, len(records), len(tree.get_terminals()))\n",
    "    #Get ACL weights\n",
    "    weights_dict, new_tree =  weighting_methods.calc_ACL_weights(tree)\n",
    "    weights_dict_id = {}\n",
    "    for i,j in weights_dict.items():\n",
    "        weights_dict_id[i.name] = j\n",
    "    #Mean scale\n",
    "    mean_ACL = np.mean(list(weights_dict_id.values()))\n",
    "    keys, vals = zip(*list(weights_dict_id.items()))\n",
    "    vals = np.array(vals)/mean_ACL\n",
    "    new_dict = dict(zip(keys, vals))\n",
    "    with open(weight_output_dir+'{}_ACL_{}.RelTime.weights'.format(prot_name, 'meanScale'),'w') as outfile:\n",
    "        for record in records:\n",
    "            outfile.write('{}\\n'.format(new_dict[record.id]))\n",
    "    print(np.sum(list(new_dict.values())))  \n",
    "\n",
    "    #Max scale\n",
    "    max_ACL = np.max(list(weights_dict_id.values()))\n",
    "    keys, vals = zip(*list(weights_dict_id.items()))\n",
    "    vals = np.array(vals)/max_ACL\n",
    "    new_dict = dict(zip(keys, vals))\n",
    "    with open(weight_output_dir+'{}_ACL_{}.RelTime.weights'.format(prot_name, 'maxScale'),'w') as outfile:\n",
    "        for record in records:\n",
    "            outfile.write('{}\\n'.format(new_dict[record.id]))\n",
    "    print(np.sum(list(new_dict.values())))  \n",
    "\n",
    "    ### Write corresponding uniform for max scale\n",
    "    weight_val = np.sum(list(new_dict.values()))/len(records)\n",
    "    with open(weight_output_dir+'{}_ACL_maxScale_uniform.RelTime.weights'.format(prot_name),'w') as outfile:\n",
    "        for record in records:\n",
    "            outfile.write('{}\\n'.format(weight_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Read in output log information and write the corresponding uniform weights for simple sequence re-weighting methods\n",
    "\n",
    "This information ultimately did not make it into the manuscript"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fasta_directory = '../Data/psicov150_aln_pdb/aln_fasta_max1k/'\n",
    "\n",
    "weight_output_dir = '../Data/weights/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('../Results/CCMPredPy_MetaData.log', 'r') as infile:\n",
    "    lines = infile.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for line in lines[:]:\n",
    "    temp_dicty = json.loads(line.strip('#>META> '))\n",
    "    outfile = temp_dicty['workflow'][0]['contact_map']['matfile']\n",
    "    if 'simple' in outfile:\n",
    "        prot_name = outfile.split('.')[0]\n",
    "        weighting_val = outfile.split('_')[1].strip('.mat')\n",
    "        neff = temp_dicty['workflow'][0]['msafile']['neff']\n",
    "        fasta_file = fasta_directory+'{}.fasta'.format(prot_name)\n",
    "        records = list(SeqIO.parse(fasta_file, 'fasta'))\n",
    "        ### First write basic uniform weights    \n",
    "        weight_val = neff/len(records)\n",
    "        with open(weight_output_dir+'{}_simple_{}_uniform.weights'.format(prot_name, weighting_val),'w') as outfile:\n",
    "            for record in records:\n",
    "                outfile.write('{}\\n'.format(weight_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finally writing my own version of simple sequence weights as well as a slightly more complicated version that I call similarity-adjusted\n",
    "\n",
    "The code is slow / inefficient, and I'm sure there is a better solution here but I'm relatively unconcerned at the moment having ensured that it gives proper results.\n",
    "\n",
    "Note that you'll want to run this for variable `id_param` values (0.1 to 0.9 show up in the manuscript) and just because it's how I initially wrote it, `id_param` is 1 - the lambda parameter that shows up in the manuscript so the math might look a bit strange here "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fasta_directory = '../Data/psicov150_aln_pdb/aln_fasta_max1k/'\n",
    "tree_directory = '../Data/psicov150_aln_pdb/aln_fasta_max1k_iqtree/'\n",
    "\n",
    "weight_output_dir = '../Data/weights/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "id_param = 0.7\n",
    "\n",
    "for fasta_file in sorted(glob.glob(fasta_directory+'*.fasta'))[:]:\n",
    "    #Read in sequence data and tree\n",
    "    records = list(SeqIO.parse(fasta_file, 'fasta'))\n",
    "    prot_name = fasta_file.split('/')[-1].split('.')[0]\n",
    "#     tree_loc = tree_directory+'{}.fasta.treefile'.format(prot_name)\n",
    "#     try:\n",
    "#         tree = Phylo.read(tree_loc, rooted=False, format='newick')\n",
    "#     except FileNotFoundError:\n",
    "#         continue  \n",
    "    \n",
    "    ###Initialize the weights dictionaries so each sequence gets a value of 1\n",
    "    weights_dict_basic = {}\n",
    "    for record in records:\n",
    "        weights_dict_basic[record.id] = 1\n",
    "        \n",
    "    weights_dict_complex = {}\n",
    "    for record in records:\n",
    "        weights_dict_complex[record.id] = 1\n",
    "    \n",
    "    ###Gross double for loop \n",
    "    seq_len = len(str(records[0].seq))\n",
    "    for i, record_i in enumerate(records):\n",
    "        for j in range(i+1, len(records)):\n",
    "            ###Get the hamming distance between the strings (divided by the length)\n",
    "            scaled_dist = Levenshtein.hamming(str(record_i.seq), str(records[j].seq))/seq_len\n",
    "            if scaled_dist <= id_param:\n",
    "                weights_dict_basic[record_i.id] += 1\n",
    "                weights_dict_basic[records[j].id] += 1                \n",
    "                weights_dict_complex[record_i.id] += (id_param-scaled_dist)/id_param\n",
    "                weights_dict_complex[records[j].id] += (id_param-scaled_dist)/id_param\n",
    "\n",
    "    \n",
    "    for key, val in weights_dict_basic.items():\n",
    "        weights_dict_basic[key] = 1./val\n",
    "        \n",
    "    for key, val in weights_dict_complex.items():\n",
    "        weights_dict_complex[key] = 1./val\n",
    "            \n",
    "    with open(weight_output_dir+'{}_simple_{}.weights'.format(prot_name, round(1.0-id_param,2)),'w') as outfile:\n",
    "        for record in records:\n",
    "            outfile.write('{}\\n'.format(weights_dict_basic[record.id]))\n",
    "            \n",
    "    with open(weight_output_dir+'{}_simpleish_{}.weights'.format(prot_name, round(1.0-id_param,2)),'w') as outfile:\n",
    "        for record in records:\n",
    "            outfile.write('{}\\n'.format(weights_dict_complex[record.id]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checking correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "prot_name = '1aoeA'\n",
    "# a = np.genfromtxt(weight_output_dir+'{}_GSC_meanScale.weights'.format(prot_name))\n",
    "# b = np.genfromtxt(weight_output_dir+'{}_ACL_meanScale.weights'.format(prot_name))\n",
    "\n",
    "a = np.genfromtxt(weight_output_dir+'{}_simple_0.8.weights'.format(prot_name))\n",
    "b = np.genfromtxt(weight_output_dir+'{}_simpleish_0.8.weights'.format(prot_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SpearmanrResult(correlation=0.9852889085534186, pvalue=0.0)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAHWpJREFUeJzt3X+M1OWdB/D3Z4dZXCiyWhbPLuDShmJFrOgetiG5alsK\n1RaInCKp6XkxmvbO5no2e8FALNj29I47z7vEu5YmptdqFQG7WU9a4lWMCXE5llthCyctUguMpmyr\nSxtZZRg+98fMrDOz3+98n9n5/pjnmfcrIdl55rszz9ddP/vM53mezyOqCiIicktL0h0gIqLwMbgT\nETmIwZ2IyEEM7kREDmJwJyJyEIM7EZGDGNyJiBzE4E5E5CAGdyIiB01K6o1nzJihXV1dSb09EZGV\n9u/f/ztV7Qi6LrHg3tXVhYGBgaTenojISiLyG5PrmJYhInIQgzsRkYMY3ImIHMTgTkTkIAZ3IiIH\nBa6WEZHHAHwBwClVvdLjeQHwrwBuBHAGwB2q+r9hd5SIyHZd654b1/b6QzdF8l4mI/cfAFhe5fnP\nA5hX+Hc3gP+ov1tERG7xCuzV2usVOHJX1ZdEpKvKJSsB/FDz5/X1i0i7iFyqqm+G1Eei0F33nefx\n2z+eHXt8ybRW7F2/NMEeEYUrjJx7J4ATJY9PFtrGEZG7RWRARAaGh4dDeGui2lUGdgD47R/P4rrv\nPJ9Qj4jCF+uEqqpuUdVuVe3u6AjcPUsUicrAHtROZKMwyg9kAMwueTyr0EYO6R3MYPOuI3hjZBQf\nam9Dz7L5WLXI8wMaETWAMEbufQC+LHmfAHCa+Xa39A5mcN8zQ8iMjEIBZEZGcd8zQ+gd5N9wIlN+\nq2KiWi1jshTySQDXA5ghIicBfBNAGgBU9bsAdiK/DPIo8ksh/zKSnlJiNu86gtFsrqxtNJvD5l1H\nrBy9XzKt1TMFc8m01gR6Q80kqkDuxWS1zNqA5xXAX4fWI2o4b4yM1tTe6PauX+q5/IyrZcgl3KFK\ngT7U3lZTe6O76ps/q6mdyEYM7hSoZ9l8tKVTZW1t6RR6ls1PqEf1+cN7uZraiWyU2GEdZI9iXp2r\nZYjsweBORlYt6mQwJ7II0zLUdC6cnKqpnchGHLmTEZc2MS2cNR17XnvLs53IFQzuFKi4iam41r24\niQmAlQHeK7BXayeyEdMyFKjaJiYiakwM7hTItU1MRM2AwZ0CubaJad7MqTW1E9mIwZ0C9Sybj3SL\nlLWlW8TaTUxnzp6vqZ3IRgzuZEYCHluEaSZqBgzuFGjzriPI5rSsLZtTaydUXUszEXlhcKdAro10\n/dJJtqaZiLwwuFMg10a6D+48XFM7kY0Y3CmQa1UheYYqNQPuUKVArApJZB8GdzLCqpBEdmFahojI\nQRy5kxGXqkISNQMGd8vFEXRdqwpJ1AyYlrFYMehmRkaheD/o9g5mQn0fVoUksg+Du8XiCrqubWLy\nq5xgcUUFonEY3C0WV9B1bRNTi3iHcb92IhsxuFssrqB7w+UdNbU3upxqTe1ENmJwt1hcQXf3q8M1\ntTe6lM8I3a+dyEYM7haLK+i6lnOf2ur9a+/XTmQj/jZbLK6gO70tXVN7o/vDe7ma2olsxOBusbhy\n7n7ZCmYxiBoXg7vF4qrWOHImW1M7ESWPwd1iqxZ14sGbF6KzvQ0CoLO9DQ/evDD0XaOuLYW8IOX9\nkcOvnchGLD9guTiqNd5weQce7z/u2W6jd3PeSx792olsZDRyF5HlInJERI6KyDqP5+eIyG4RGRSR\ngyJyY/hdpaS4thSSqBkEBncRSQF4FMDnAVwBYK2IXFFx2QYAT6vqIgC3Afj3sDtKyXFtKSRRMzAZ\nuS8GcFRVj6nqWQBPAVhZcY0CuLDw9XQAb4TXRUqaazn3JR+5uKZ2IhuZBPdOACdKHp8stJXaCOB2\nETkJYCeAr4XSOwrUO5jBkodewNx1z2HJQy+EXhEScO8M1Sfu+uS4QL7kIxfjibs+mVCPiMIX1mqZ\ntQB+oKqzANwI4EciMu61ReRuERkQkYHhYeZr6xVXyd+4VuXEaW7HB8bKDaREMLfjAwn3iChcJsE9\nA2B2yeNZhbZSdwJ4GgBU9WUAFwCYUflCqrpFVbtVtbujw86VFo0kzjrr2waOl/0R2TYwfvWMLTb0\nDuHx/uNjhcJyqni8/zg29A4l3DOi8JgE930A5onIXBFpRX7CtK/imuMAPgMAIvIx5IM7h+YRi2ui\n80vffxl7XnurrG3Pa2/hS99/OdT3icuTe0/U1E5ko8DgrqrnANwDYBeA/0N+VcwhEXlARFYULvsG\ngLtE5ACAJwHcocr6qVGLa6KzMrAHtTc6lvylZmC0iUlVdyI/UVradn/J14cBLAm3axSkZ9n8srNN\nAbsnOokoPNyharHihGbUB2QTkX1YW4YCpX1+S/zaiSh5HLlbrLgUspiWKS6FBBDq6P3c+draiSh5\nHHtZLK6lkK7tUPUr/siikOQSBneLxbUU0rUdqud9FsX4tRPZiMHdYu1TvI+582ufKNd2qLb5TBb4\ntRPZiDl3i/kty45iuXYcdePjMuozWeDXTmQjDlUsdnrU+5g7v3bKi/OPIlFSGNwtFldaxjUpn5O9\n/dqJbMTgbjGOQCdm7XWza2onshFz7hYb8Um/+LVT3rdXLQSQLxSWU0VKBGuvmz3WTuQCBveE9Q5m\nJlw+ICXiWeyK6YVg3161kMGcnMbgnqB6d5iyuiER+WFwT1C1HaYmwb2zvQ0Zjw1LnZbuHI3T0odf\nxK9OvTP2eN7MqXj+3uuT6xBRyDihmqB6d5jGuXM0jrNa41IZ2AHgV6fewdKHX0ymQ0QRYHBPUL01\nW1Yt6sQ1c6aXtV0zZ3rom43iOqs1LpWBPaidyEYM7gnqWTYf6YpqVemUGI+8N/QOeR5/F/ZZoHGe\n1UpE4WBwT1rl3GcNc6FxnQXqldev1k5EyWNwT9DmXUeQrShFmD2vxiPiuFbLuLajc97MqTW1E9mI\nwT1B9U6oxhV0ueSSyD4M7gmqd0I1rm30fksrbV1yyQlVagYM7gmqdylj92UXo6VikN4i+fYwuXZY\nB1EzYHBPUL2HYGzedWTc6UHnFaGvYnHtsA6iZsAdqgmr5xCMuI7ZA9w6rIOoGXDkbjHXDq4movAw\nuFuMuXAi8sO0jMWKaZKJlgyuRT2liYkofgzulosjF15vaeJGwzr41AwY3BNmw4i43tLEjYabsqgZ\nMLgnyJYRMWvLENmHE6oJsqXaomu1ZYiaAUfuCQpjnfqG3qHID3pmGoPIPhy5J6jedeobeofweP/x\nsSCbU8Xj/cdDr+fuWm0Zv88b/BxCLjEK7iKyXESOiMhREVnnc82tInJYRA6JyI/D7WbjCePYuXrX\nqcdVz9219fTc/EXNIDAtIyIpAI8CWArgJIB9ItKnqodLrpkH4D4AS1T1bRGZGVWHG0FYE6H1rlOP\nK10S53r6OMRZtoEoKSY598UAjqrqMQAQkacArARwuOSauwA8qqpvA4Cqngq7o3ExWZoY5tJAW2q2\n2NJPEx9qb/Nc6cORO7nEJLh3Aij9nH8SwHUV13wUAERkD4AUgI2q+rNQehgj0xF5M478bFiPb6rr\ng97BveuDDO7kjrAmVCcBmAfgegBrAXxfRNorLxKRu0VkQEQGhoeHQ3rr8JguTWyUnG1cE53FP3qZ\nkVEo3v+jN5F5hkbQf+ztmtqJbGQS3DMASo/2mVVoK3USQJ+qZlX11wB+iXywL6OqW1S1W1W7Ozo6\nJtrnyJiOyBtlgjGuftiyHt8Ul3ZSMzAJ7vsAzBORuSLSCuA2AH0V1/QiP2qHiMxAPk1zLMR+xsJ0\nRN4oh1fE1Y9mTEMR2S4w566q50TkHgC7kM+nP6aqh0TkAQADqtpXeO5zInIYQA5Aj6r+PsqOR6Fn\n2fyynDvgPxJulAnGOPrBCUgi+xjtUFXVnQB2VrTdX/K1Ari38M9ari35C8uUVu8PeH7tRJQ8lh+o\nENZI2KXVJb869U5N7USUPAb3CNhS7ZGI3MXP1RFwbXUJEdmHwT0Crq0umTzJ+9fEr52Iksf/OyPQ\nKJucwnL23Pma2okoeQzuEWiUTU5hmdKaqqmdiJLXFBOqca9cqWVJpQ2ras6czdXUTkTJcz64J7Vy\nxWRJZe9gBj3bDyCb07G+9Ww/UFPf4vjj4Lcpn5v1iRqX82mZpFaumBzmsenZQ2OBvSibU2x69pDx\ne8RR0IsnFxHZx/mRu8nKlbBHv6afFt4+k/X8fr/2SmHWla9mSmsK73ikYJhzJ2pczo/cg1auRDH6\njevTQlxLLr0Ce7V2Ikqe88E9aOVKFIHYNOi2t6U9r/Nrr+TakksiCo/zwT2oLG4Uo1/ToLtxxQKk\nW8oz1+kWwcYVC4zex7Ull0QUHudz7kD1lStRlLM1LR1cbxVKVrEkIj9NEdyrqaWGu6lagm69VSgb\npa48ETWWpg/uUY1+XQq6Sz5yMfa89pZnOxE1pqYP7oBbgTgKr//ee/7Br73RzZs51bMW/byZUxPo\nDVE0nJ9QdZ3JZql6ec1JVGtvdM/fe/24QD5v5lQ8f+/1yXSIKAIcuVuMh4JMHAM5uY4jd4vxUBAi\n8sORe8LqKX3g2qEgRBQejtwTVG/pA+5QJSI/DO4Jqjetwh2qROSHaZkE1ZtWiWuHaroFyHqcqJfm\n0ICoYTG4JyiM0gdxrNH3CuzV2okoeRx7JYhpFSKKCoN7glYt6sTqazuRknxlyJQIVl/L3bJEVD/r\n0zJRnyEa5ev3DmawY38GOc0ftZdTxY79GXRfdjEDPBHVxeqRe9RniEb9+tyERERRsTq4Rx0co379\nMDYhxVFbhojsY3VaptbgWGuKpZ7ga/Je9a6WYW0ZIvJj9ci9lh2aE0mxTHQHqOl71btahmkdIvJj\ndXCvJThOJBBONPiavle9q2VYW4aI/BgFdxFZLiJHROSoiKyrct1qEVER6Q6vi/6CDr8uVS0Q+uWt\na3l90/cq5bdahrVliKhegTl3EUkBeBTAUgAnAewTkT5VPVxx3TQAfwNgbxQd9WO6Q9Mvvz29LV01\nbz2RHaCmufRqI3yT94zi/FcicoPJyH0xgKOqekxVzwJ4CsBKj+u+BeAfALwbYv9C45diEUHoeWvT\ndE69JxxN9JMFEbnPZLVMJ4ATJY9PAriu9AIRuQbAbFV9TkR6/F5IRO4GcDcAzJkzp/be1sGvyNbf\nbn3F8/p68tamBb1SImMpmcr2Wt6LwZyIKtW9FFJEWgA8DOCOoGtVdQuALQDQ3d09PqpFzCsQbt51\npO7iXabvVckrsFdr9xL1Dl0ispNJcM8AmF3yeFahrWgagCsBvCj5EeefAOgTkRWqOhBWR2vhF/C8\n2pPMW3f65OY7a1jn3rP9ALK5/B+DzMgoerYfAMB17kTNziTnvg/APBGZKyKtAG4D0Fd8UlVPq+oM\nVe1S1S4A/QASDexea8w39A55tgNILG99w+UdNbVX2vTsobHAXpTNKTY9e6juvhGR3QJH7qp6TkTu\nAbALQArAY6p6SEQeADCgqn3VXyFefitQntx7Yly6ozhxumfdp0MP5ibpkt2vDnt+r197pbfPZGtq\nJ6LmYZRzV9WdAHZWtN3vc+319Xdr4vwmQv3y2FFs+OkdzKBn2wFkz5ekS7aNT5dwExIRRcXqHape\n/CZC/VagRLHhZ2PfobHAXpQ9r9jYV54uaZ+S9vx+v3YiIlPOBXe/NeZrr5sd26lHI6PeaZHK9ncr\n0kdB7UREpqyuCuml2hrz7ssubqhlg6M+h5D6tRMRmXIuuAP+a8y54YeImoVzaZlG4Le/1HzfKRFR\nfRjcI+C3vzT2LblE1LQY3CPgtzKnsr29zWe1jE97Jb+drKY7XInIXQzuETCtGbNxxQKkW8oDfrpF\nsHHFAqP3qXeHq6kLJ6dqaiei5DG4R8B0RL5qUSfWLJ5ddhLTmsWzjSd9d+w/WVP7RB3ctHxcIL9w\ncgoHNy0P9X2IKDxOrpapRxhVFv0q9la29w5m8ET/8bFcfE4VT/QfR/dlFxu9Z5xLKRnIiezC4F6i\nWHSs9FSmnu0HsLHvEE6PZo2D/YhPbZfK9r/bfmDcJKsW2rlkk4jqwbRMCa+iY9mcYmQ0W1ZJMuiM\nU9OzTc/mvHPzfu1ERKaaIrj7HYBdyaRgl8kRfKbH7BERRcX5tIxXqqX0AOxSfgdbVwq6xvSYPSKi\nqDgf3P3qu2/edWRcsL3h8g483n888DVNzjg1KXWQEsArA5PiVlYiqpNzwb1ytYvfKNsrBfPcwTeN\n3qOWM06rv05t7UREppwK7l4pGIH3tn+vSU/TE4waZQdougXwWvWYboqZFCKqxqkw4JWCUYwv2FXP\n5Kbp95pO4tZjzeI5NbUTUfNwauTut9pFkc+T51SREsHqa73z4e1tac+DNop/HEwnRnsHM/jGtgPI\nlRyz9w2PY/bqVe8ZrETkLuuDe2mOvaUQwCsJ3s+T51SxY3/GcxfoxhULys4+BfK1Xjbf8vGagvL6\nnwyNBfai3HnF+p8Mlb1OvWkVv/kEkxU/ROQ2q9MyxRx7ZmQUCu+JTq+cu99ada9aL4vnXoTNu47U\nlF5556z3MXmV7VMne9eg8WsnIjJl9cjdK8cO5IPyedWqq2UyI6NY8tALZevQAWDH/kzZKH/Pa2+V\nfU/P9vDSK6ZnrRIR1crq4O6XY8+porO9DW+MjI7l2isJ3k9fFDc2XZBu8fxjUSqbU2x69hA3JBFR\nQ7M6LeNXw6UYuGtN1ZguhTS9LmpTfJLzfu1E1DysjgJeNVz81rWnRCDIr1GPeo9QvScsmfr7m69C\nxVkfaJF8OxE1N6uD+6pFnXjw5oXobG8LDNy5Qg6+Z9l8301IYY14v/DxS43a6z1Ie9WiTjx869Vl\n9//wrVczZUREdufcgfE1XJY89ELVSdT7nhnC6ms7sWN/piy/3pZOoXVSC86EcNCFXxmD5w6+iW+v\nWjj2OIyDtE1q2BBR87F65O7lhss7qo58R7M57H51GKuv7Sxb8rj62k6cNlylctGU6ukVv5x8ZTsP\nuCaiqDgV3HsHM9ixPxM48s2MjI5b8rhjfwbTDXPiN13lnXapVc+y+Z4HZLPuOxHVy6ng7rfuvZII\nPMsAi2DcBK2X/zpQvXpkLROq2YqdrJWPiYgmwqngbnKSEgD4VewdOZMtS9f4GRnNVi0MtnHFAs8R\n+cYVC8raera94vn6fu1ERKasn1AtNd2n8Jep9ilpbN13wqhee2kNmszIKHpKCoOZnsTkN3cbwpwu\nETU5p4L72XPBKRkgnx5579z5stSMoLbNSV7plI197+9c5SoWIkqSUVpGRJaLyBEROSoi6zyev1dE\nDovIQRH5uYhcFn5Xg5ksY2xLp7BxxYKx9fGA/8anWrEmDBE1isDgLiIpAI8C+DyAKwCsFZErKi4b\nBNCtqlcB2A7gH8PuaL2Km3wevHnh2Kh6z7pPx7Jj1Y/fnilWDyCiepmkZRYDOKqqxwBARJ4CsBLA\n4eIFqrq75Pp+ALeH2UlTF01Je6ZWLpqSxuD9n/P8HtNJ2FpVnuXqlXPffMvV+PrW8ZOnm2+5OpI+\nEVHzMBkjdgI4UfL4ZKHNz50Afur1hIjcLSIDIjIwPBz+aUHf/OICpFMVq1RSgm9+cYHPd/gXH6tH\n72AGX9/6yljxsszIKL6+9ZVxteBXLerEI2vKywc8soblA4iofqFOqIrI7QC6AXzK63lV3QJgCwB0\nd3eHkg2pHCGv+dPZ2P3qcNURc6meZfPLDtUOg9dovNhe2RdOvBJRFEyCewbA7JLHswptZUTkswDW\nA/iUqr4XTvf89Q5msOnZQ2VpmOLO02Je3UTpskUeT0dErjBJy+wDME9E5opIK4DbAPSVXiAiiwB8\nD8AKVT0VfjfLFY/X88qvVx6hV22zUVFxcjWoZgwRkS0CR+6qek5E7gGwC0AKwGOqekhEHgAwoKp9\nADYD+ACAbZLf3XlcVVdE1emgMgPFI/RuuLwDW/edQDZXstmoyjF5N111KR7vP171vVMtMu7wayKi\nRmOUc1fVnQB2VrTdX/L1Z0PuV1Um6ZPMyKhnoK52TN7T+06MaysSYCyH75dTJyJqFE7tUDX19pms\n51LFszn/EfmvH7pp7Gu//HxxU9SkFsE5j9H9pMpjk4iIItK022Xue2aobKnifc8MGX+v1/F+benU\nWKnef7rl4+NqykuhnYgoDk05cge8S/6aCioMZlo4jIgoKk0b3OsVtD6d69eJKElNm5YhInJZ0wZ3\nTm0SkcuaNrgreBA1EbnLyuAedAyeic72NuxZ9+kQekNE1HisDO4mx+AFKS5bNGHlfyQiampWxq24\n0yksNkBEtrEyuHttIqpVaXGxIAzuRGQbK4P7qkWdZWegTkSxfMAl01rD6hYRUcOwMrgD75fprXdy\nde/6pQzwROQca4N7URiTq3vXL8XrJYXBiIhsZ31wD3Ny9ZE13gdT+7UTETUqK2vLlJbrnd6WntAB\nGhekxqdzWPCLiFxhXXAvHrFXrOI4MppFukVwQWsK75w1r+z46ndu9GxnwS8icoF1aRmvI/ay5xXt\nU1rx+kM34ZE1V6OzvY21Y4ioqVk3cvc7Yq/YXjry7lr3XGz9IiJqJNaN3KstfdzQW36akt+VHNUT\nkeusCu4beoeqLn2sPBD7X3xWufi1ExG5wpq0zIbeoXHBOwhXvxBRs7ImuNca2Iu4+oWImpFVaRki\nIjLD4E5E5CCngrvXrlMiombkTHC/ICW+u06JiJqNNROqF05O4Q/vjS8vcOHkFA5uWp5Aj4iIGpc1\nI/eDm5bjwsnlpy8xsBMRebNm5A6AgZyIyJA1I3ciIjLH4E5E5CAGdyIiBzG4ExE5iMGdiMhBolVK\n6Eb6xiLDAH5T0TwDwO8S6E4j4L03J957c6rn3i9T1Y6gixIL7l5EZEBVu5PuRxJ477z3ZsN7j/be\nmZYhInIQgzsRkYMaLbhvSboDCeK9Nyfee3OK/N4bKudOREThaLSROxERhSCR4C4iy0XkiIgcFZF1\nHs9PFpGthef3ikhX/L2MhsG93ysih0XkoIj8XEQuS6KfUQi695LrVouIiogzKylM7l1Ebi387A+J\nyI/j7mNUDH7n54jIbhEZLPzeO3Ewg4g8JiKnROQXPs+LiPxb4b/LQRG5JtQOqGqs/wCkALwG4MMA\nWgEcAHBFxTV/BeC7ha9vA7A17n4meO83AJhS+PqrzXTvheumAXgJQD+A7qT7HePPfR6AQQAXFR7P\nTLrfMd77FgBfLXx9BYDXk+53SPf+ZwCuAfALn+dvBPBTAALgEwD2hvn+SYzcFwM4qqrHVPUsgKcA\nrKy4ZiWA/yx8vR3AZ0TEhTP0Au9dVXer6pnCw34As2LuY1RMfu4A8C0A/wDg3Tg7FzGTe78LwKOq\n+jYAqOqpmPsYFZN7VwAXFr6eDuCNGPsXGVV9CcBbVS5ZCeCHmtcPoF1ELg3r/ZMI7p0ATpQ8Pllo\n87xGVc8BOA3gg7H0Llom917qTuT/srsg8N4LH0tnq+pzcXYsBiY/948C+KiI7BGRfhFx5fACk3vf\nCOB2ETkJYCeAr8XTtcTVGg9qYtVhHc1ERG4H0A3gU0n3JQ4i0gLgYQB3JNyVpExCPjVzPfKf1l4S\nkYWqOpJor+KxFsAPVPWfReSTAH4kIleq6vmkO2azJEbuGQCzSx7PKrR5XiMik5D/qPb7WHoXLZN7\nh4h8FsB6ACtU9b2Y+ha1oHufBuBKAC+KyOvI5yD7HJlUNfm5nwTQp6pZVf01gF8iH+xtZ3LvdwJ4\nGgBU9WUAFyBfe8V1RvFgopII7vsAzBORuSLSivyEaV/FNX0A/qLw9Z8DeEELMxCWC7x3EVkE4HvI\nB3ZX8q5AwL2r6mlVnaGqXarahfx8wwpVHUimu6Ey+Z3vRX7UDhGZgXya5licnYyIyb0fB/AZABCR\njyEf3Idj7WUy+gB8ubBq5hMATqvqm6G9ekKzyDciPzJ5DcD6QtsDyP/PDOR/uNsAHAXwPwA+nPTM\nd4z3/t8AfgvglcK/vqT7HNe9V1z7IhxZLWP4cxfk01KHAQwBuC3pPsd471cA2IP8SppXAHwu6T6H\ndN9PAngTQBb5T2Z3AvgKgK+U/MwfLfx3GQr79507VImIHMQdqkREDmJwJyJyEIM7EZGDGNyJiBzE\n4E5E5CAGdyIiBzG4ExE5iMGdiMhB/w8OXXEEur/YlQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11c403080>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(a, b, marker='o', linestyle='')\n",
    "stats.spearmanr(a,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.5"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "12px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
