{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview\n",
    "\n",
    "This describes how I processed the psicov dataset files to my liking: http://bioinfadmin.cs.ucl.ac.uk/downloads/contact_pred_datasets/\n",
    "\n",
    "The `.tar` file that you download here should contain `aln` and `pdb` directories.\n",
    "\n",
    "When we're done with our data cleaning, we're going to add an `aln_fasta` directory since the original alignment files are just in the psicov format. \n",
    "\n",
    "We'll also have a separate `aln_fasta_max1k` directory where no alignment will have more than 1001 sequences and the reference sequence will always be included in the set.\n",
    "\n",
    "Offline we'll make some trees using FastTree2 which will reside in an eventual `raw_trees` directory.\n",
    "\n",
    "Those trees will be rooted with the midpoint algorithm and put in `mp_root_trees` and then sent through IQTree and put in the `aln_fasta_max1k_iqtree` directory.\n",
    "\n",
    "So you'll know you've done something right if at the end of all the data processing you have residing in some folder of interest:\n",
    "\n",
    "1. `pdb/`\n",
    "2. `aln/`\n",
    "3. `aln_fasta/`\n",
    "4. `aln_fasta_max1k/`\n",
    "5. `raw_trees/`\n",
    "6. `mp_root_trees/`\n",
    "7. `aln_fasta_max1k_iqtree/`\n",
    "\n",
    "My code assumes that these will all reside in `../Data/psicov_aln_pdb/`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**First the basic imports**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "from Bio import SeqIO, Phylo\n",
    "\n",
    "###I didn't use a random seed so your final results are going to be slightly different\n",
    "###if you step through this entire pipeline from scratch. Alternatively you can just use the\n",
    "###files I computed \n",
    "import random "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Some globally relevant variable/s**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pdb_directory = '../Data/psicov150_aln_pdb/pdb/' #Make sure that replacing \"pdb/\" with \"aln/\" exists!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean `.fasta` files up a bit \n",
    "**(and truncate to max 1k seqs)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def replace_all(string, char_list_to_replace):\n",
    "    \"\"\"\n",
    "    Just a basic and slow function to replace some weird characters should they appear\n",
    "    \"\"\"\n",
    "    for char in char_list_to_replace:\n",
    "        if char in string:\n",
    "            string = string.replace(char, '-')\n",
    "    return string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "############\n",
    "invalid_characters = ['X', 'Z', 'B', 'U']\n",
    "\n",
    "for pdb_file in glob.glob(pdb_directory+'*.pdb')[:]:\n",
    "    print(pdb_file)\n",
    "    pdb_seqio = list(SeqIO.parse(pdb_file, format='pdb-atom'))\n",
    "    assert len(pdb_seqio) == 1\n",
    "    #Grab the sequence!\n",
    "    pdb_seq = str(pdb_seqio[0].seq)\n",
    "    #Replace any of the above invalid characters\n",
    "    pdb_seq = replace_all(pdb_seq, invalid_characters)\n",
    "    #This file should exist from the original psicov directory\n",
    "    aln_file = pdb_file.replace('/pdb/', '/aln/').replace('.pdb', '.aln')\n",
    "    with open(aln_file, 'r') as infile:\n",
    "        aln_seqs = infile.readlines()\n",
    "        aln_seqs = [i.strip('\\n') for i in aln_seqs]\n",
    "        aln_seqs = [replace_all(i, invalid_characters) for i in aln_seqs]\n",
    "    #We'll temporarily remove the reference sequence\n",
    "    if pdb_seq in aln_seqs:\n",
    "        aln_seqs.remove(pdb_seq)\n",
    "\n",
    "    ####################################\n",
    "    ###This shouldn't happen. And yet...\n",
    "    if len(pdb_seq) != len(aln_seqs[0]) or len(pdb_seq) != len(aln_seqs[-1]):\n",
    "        print('{} had an error'.format(pdb_file))\n",
    "        continue\n",
    "    ####################################\n",
    "\n",
    "    \n",
    "    \n",
    "    ###Now we're writing the full alignments to fasta files with just numbers as their name\n",
    "    with open(aln_file.replace('/aln/', '/aln_fasta/').replace('.aln', '.fasta'), 'w') as outfile:\n",
    "        #First write the reference sequence\n",
    "        outfile.write('>{}\\n{}\\n'.format('reference', pdb_seq))\n",
    "        #Now everyone else\n",
    "        for i, aln in enumerate(aln_seqs):\n",
    "            outfile.write('>{}\\n{}\\n'.format(i, aln))\n",
    "    \n",
    "    #Sample the original sequences if there are too many of them\n",
    "    if len(aln_seqs) > 1000.:\n",
    "        aln_seqs = random.sample(aln_seqs, 1000)\n",
    "    with open(aln_file.replace('/aln/', '/aln_fasta_max1k/').replace('.aln', '.fasta'), 'w') as outfile:\n",
    "        #As before first write the reference\n",
    "        outfile.write('>{}\\n{}\\n'.format('reference', pdb_seq))\n",
    "        #And now the remaining sequences\n",
    "        for i, aln in enumerate(aln_seqs):\n",
    "            outfile.write('>{}\\n{}\\n'.format(i, aln))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Offline: Run `FastTree`\n",
    "\n",
    "**An important caveat here is that (I believe) that `FastTree` should be run in double precision mode. Which is to say, the normal `FastTree` probably is not the best for resolving such large alignments so make sure to run `FastTreeDbl`**\n",
    "\n",
    "**You'll want to put these `.newick` tree results for each protein family into `raw_trees/`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Back online: Root `FastTree` results\n",
    "\n",
    "The actual rooting algorithm *probably* isn't too big of a deal here so I'm sticking with the old standby of mid-point rooting. Though I'd note that I made a better/faster implementation than the stock BioPython version. And in the future it couldn't hurt to root these trees using a more complex algorithm.\n",
    "\n",
    "The other important caveat here is that you need to put your FastTree newick output files into `raw_trees`. This will give them a quick root."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import supporting_functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for fasttree_file in glob.glob('../Data/psicov150_aln_pdb/raw_trees/*.newick'):\n",
    "    tree = Phylo.read(fasttree_file, 'newick')\n",
    "    rooted_tree = supporting_functions.MP_root(tree)\n",
    "    Phylo.write(tree, fasttree_file.replace('/raw_trees/', '/mp_root_trees/'),\\\n",
    "                'newick', format_branch_length='%1.16f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Offline: Run `IQTree`\n",
    "\n",
    "**You'll want to put these results in `aln_fasta_max1k_iqtree/`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**fin.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "12px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
